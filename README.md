# ML Assignment â€“ Probabilistic Models, Decision Trees & Random Forests

This repository contains complete from-scratch implementations of:

- Gaussian Generative Classifier (LDA-style)
- Gaussian Naive Bayes
- Decision Tree Classifier for continuous features
- Random Forest using custom decision trees

The project follows a full ML pipeline: dataset preparation, stratified splitting,
hyperparameter tuning, evaluation, metrics, visualization, and model comparison.

---

## ğŸš€ Project Structure

decision-trees-random-forest_from_scratch_probabilistic-classifier/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ data_split.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ plots.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ gaussian_generative.py
â”‚   â”‚   â”œâ”€â”€ naive_bayes.py
â”‚   â”‚   â”œâ”€â”€ decision_tree.py
â”‚   â”‚   â””â”€â”€ random_forest.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_gaussian.py
â”‚   â”‚   â”œâ”€â”€ train_naive_bayes.py
â”‚   â”‚   â”œâ”€â”€ train_decision_tree.py
â”‚   â”‚   â””â”€â”€ train_random_forest.py
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ evaluate_gaussian.py
â”‚       â”œâ”€â”€ evaluate_naive_bayes.py
â”‚       â”œâ”€â”€ evaluate_decision_tree.py
â”‚       â””â”€â”€ evaluate_random_forest.py
â”‚
â”œâ”€â”€ notebooks/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_tree.py
â”‚   â””â”€â”€ test_nb.py
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_all.py


---

## ğŸ” Implemented Models

### **1. Gaussian Generative Classifier**
- Estimates class priors, means, shared covariance
- Uses regularized Î£ + Î»I
- Tuned over Î» âˆˆ {1eâˆ’4, 1eâˆ’3, 1eâˆ’2, 1eâˆ’1}

### **2. Naive Bayes (Categorical)**
- Laplace smoothing Î± âˆˆ [0.1, 5]
- Comparisons with sklearnâ€™s MultinomialNB
- Full probability-table implementation

### **3. Decision Tree (from scratch)**
- Continuous feature splits
- Entropy & Information Gain
- Hyperparameters:
  - max_depth âˆˆ {2,4,6,8,10}
  - min_samples_split âˆˆ {2,5,10}
- Feature importance via accumulated information gain

### **4. Random Forest (Bonus)**
- Bootstrap sampling
- Random feature subsets
- Majority voting
- Hyperparameters:
  - T âˆˆ {5,10,30,50}
  - max_features âˆˆ {sqrt(d), d/2}

---

## ğŸ“Š Evaluation

Each model includes:

- Accuracy, Precision, Recall, F1
- Confusion matrix
- Cross-model comparison
- Biasâ€“variance analysis (Tree vs Forest)

---

## ğŸ§  Why This Project is Interesting

This project demonstrates:

- Complete ML model implementations **without sklearn**  
- Understanding of entropy, information gain, and tree construction  
- Ensemble learning and variance reduction  
- Real-world datasets (Digits, Adult, Breast Cancer)

This builds skills in:
- Mathematical modeling  
- Core ML algorithms  
- Software structuring and modular pipelines  
- Experimental analysis  

---

## ğŸ›  Requirements

numpy
pandas
scikit-learn
matplotlib


---

## ğŸ“¦ Running Everything

python run_all.py
